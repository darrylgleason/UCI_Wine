{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It should take less than about 35 s to run the\n",
    "# entire notebook under the default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import appropriate packages and set analysis options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') \n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# Setting randomize_seeding to True will\n",
    "# randomize various operations throughout\n",
    "# the notebook. Setting it to False will\n",
    "# cause the seed to remain fixed to some\n",
    "# specified value such that the notebook\n",
    "# can be reran with the same randomized\n",
    "# variables (see MAGIC_SEED below).\n",
    "randomize_seeding = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define convenient variables and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These paths indicate from where the training\n",
    "# data will be loaded.\n",
    "DATA_PATH = \"./data/wine.data\"\n",
    "\n",
    "# The MAGIC_SEED optionally specifies a fixed\n",
    "# random state/seed so that the notebook can be\n",
    "# reran with the same randomized variables (see\n",
    "# randomize_seeding above).\n",
    "MAGIC_SEED = 1776\n",
    "if (randomize_seeding):\n",
    "    MAGIC_SEED = np.random.seed()\n",
    "\n",
    "# The training data provided with this data\n",
    "# set will be split into two subsets so that\n",
    "# models can be trained on the first and tested\n",
    "# on the second. TRAINING_DATA_TEST_SIZE\n",
    "# indicates the proportion of the training\n",
    "# data that will be used as test data for\n",
    "# model evaluation and should be in the\n",
    "# range [0.0, 1.0].\n",
    "TRAINING_DATA_TEST_SIZE = 0.50\n",
    "\n",
    "def load_data(path, header_names):\n",
    "    \"\"\"\n",
    "    Load the file at 'path' into a Pandas\n",
    "    DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=None, names=header_names)\n",
    "    print(\"Loaded data dimensions: \", df.shape[0], \"rows, \", df.shape[1], \"columns\")\n",
    "    return df\n",
    "\n",
    "def print_nan(nan_cols_counts, col_type):\n",
    "    \"\"\"\n",
    "    Print each element of the list which should contain\n",
    "    a DataFrame feature name and an int number of times\n",
    "    the feature contains an NaN value.\n",
    "    \"\"\"\n",
    "    print(\"\\n\", len(nan_cols_counts), \" \", col_type, \"-type columns with NaN values.\", sep='')\n",
    "    if(len(nan_cols_counts) > 0):\n",
    "        print(\"    {:<16}{}\".format(\"Feature\", \"NaN Count\"))\n",
    "        print(\"%s\" % \"    ---------------------\")\n",
    "    for index, element in enumerate(nan_cols_counts):\n",
    "        print(\"{:>2}. {:<16}{}\".format(index+1, element[0], element[1]))\n",
    "        \n",
    "def gather_nan(df, col_type, print_if_nan = True):\n",
    "    \"\"\"\n",
    "    Find all DataFrame columns of type 'col_type'\n",
    "    which contain NaN values.\n",
    "    \"\"\"\n",
    "    if (col_type == \"int\"):\n",
    "        columns = df.select_dtypes(include=['int']).columns\n",
    "    elif (col_type == \"float\"):\n",
    "        columns = df.select_dtypes(include=['float']).columns\n",
    "    else:\n",
    "        columns = df.select_dtypes(include=['object']).columns\n",
    "    nan_cols_counts = []\n",
    "    for col in np.sort(columns):\n",
    "        num_nan = np.sum(df[col].isnull())\n",
    "        if (num_nan > 0):\n",
    "            nan_cols_counts.append((col, num_nan))\n",
    "    if (print_nan):\n",
    "        print_nan(nan_cols_counts, col_type)\n",
    "    return nan_cols_counts\n",
    "\n",
    "def evaluate_model(x_data, y_data, model, add_title = True):\n",
    "    \"\"\"\n",
    "    Given an input model and data,\n",
    "    split the data into training/testing subsets\n",
    "    and use this to produce a fit and predictions.\n",
    "    Indicate the goodness of the fit and plot\n",
    "    the results.\n",
    "    \"\"\"\n",
    "    # Split the data into two subsets.\n",
    "    # Then, train the model on the target data\n",
    "    # and use it to predict results.\n",
    "    x_data1, x_data2, y_data1, y_data2 = train_test_split(\n",
    "        x_data, y_data,\n",
    "        test_size=TRAINING_DATA_TEST_SIZE,\n",
    "        random_state=MAGIC_SEED)\n",
    "    model.fit(x_data1, y_data1)\n",
    "    y_data2_pred = model.predict(x_data2)\n",
    "    \n",
    "    # Evaluate the model & predictions by viewing\n",
    "    # the cross-validation score, error, and\n",
    "    # variance (where a variance of 1 indicates\n",
    "    # a perfect prediction) and plotting the results.\n",
    "    print(\"When using %0.1f%% of the data to perform the\"\n",
    "        \" fit and %0.1f%% of the data to make the prediction,\"\n",
    "        \" the model performed according to the following:\"\n",
    "        % (100.0*(1.0-TRAINING_DATA_TEST_SIZE), 100.0*TRAINING_DATA_TEST_SIZE))\n",
    "    print(cross_val_score(model, x_data1, y_data1, cv=5))\n",
    "    print(\"RMS Error: %.3f\"\n",
    "        % sqrt(mean_squared_error(y_data2, y_data2_pred)))\n",
    "    print('Variance score: %.3f' % r2_score(y_data2, y_data2_pred))\n",
    "    x = np.arange(6)\n",
    "    y = [y_data2.tolist().count(1), y_data2_pred.tolist().count(1),\n",
    "         y_data2.tolist().count(2), y_data2_pred.tolist().count(2),\n",
    "         y_data2.tolist().count(3), y_data2_pred.tolist().count(3)]\n",
    "    colors = (\"green\", \"cyan\", \"red\", \"magenta\", \"black\", \"gray\")\n",
    "    plt.bar(x, y, color=colors)\n",
    "    plt.xticks(x, (\"True Class1\", \"Pred. Class1\", \"True Class2\", \"Pred. Class2\",\n",
    "        \"True Class3\", \"Pred. Class3\"))\n",
    "    if add_title:\n",
    "        plt.title('k = %d' % model.get_params()['n_neighbors'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Finalize the model by fitting it to the entire data set.\n",
    "    model.fit(x_data, y_data)\n",
    "    \n",
    "    # Return a score for this model.\n",
    "    return metrics.accuracy_score(y_data2, y_data2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preview the wine data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data dimensions:  178 rows,  14 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>TotPhenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>NonflavPhenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>ColIntensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  Acid   Ash  Alcalinity  Magnesium  TotPhenols  Flavanoids  \\\n",
       "0      1    14.23  1.71  2.43        15.6        127        2.80        3.06   \n",
       "1      1    13.20  1.78  2.14        11.2        100        2.65        2.76   \n",
       "2      1    13.16  2.36  2.67        18.6        101        2.80        3.24   \n",
       "3      1    14.37  1.95  2.50        16.8        113        3.85        3.49   \n",
       "4      1    13.24  2.59  2.87        21.0        118        2.80        2.69   \n",
       "\n",
       "   NonflavPhenols  Proanth  ColIntensity   Hue    OD  Proline  \n",
       "0            0.28     2.29          5.64  1.04  3.92     1065  \n",
       "1            0.26     1.28          4.38  1.05  3.40     1050  \n",
       "2            0.30     2.81          5.68  1.03  3.17     1185  \n",
       "3            0.24     2.18          7.80  0.86  3.45     1480  \n",
       "4            0.39     1.82          4.32  1.04  2.93      735  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(DATA_PATH, ['Class', 'Alcohol', 'Acid', 'Ash', 'Alcalinity', 'Magnesium', 'TotPhenols', 'Flavanoids', 'NonflavPhenols', 'Proanth', 'ColIntensity', 'Hue', 'OD', 'Proline'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>TotPhenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>NonflavPhenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>ColIntensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.938202</td>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.775035</td>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Class     Alcohol        Acid         Ash  Alcalinity   Magnesium  \\\n",
       "count  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000   \n",
       "mean     1.938202   13.000618    2.336348    2.366517   19.494944   99.741573   \n",
       "std      0.775035    0.811827    1.117146    0.274344    3.339564   14.282484   \n",
       "min      1.000000   11.030000    0.740000    1.360000   10.600000   70.000000   \n",
       "25%      1.000000   12.362500    1.602500    2.210000   17.200000   88.000000   \n",
       "50%      2.000000   13.050000    1.865000    2.360000   19.500000   98.000000   \n",
       "75%      3.000000   13.677500    3.082500    2.557500   21.500000  107.000000   \n",
       "max      3.000000   14.830000    5.800000    3.230000   30.000000  162.000000   \n",
       "\n",
       "       TotPhenols  Flavanoids  NonflavPhenols     Proanth  ColIntensity  \\\n",
       "count  178.000000  178.000000      178.000000  178.000000    178.000000   \n",
       "mean     2.295112    2.029270        0.361854    1.590899      5.058090   \n",
       "std      0.625851    0.998859        0.124453    0.572359      2.318286   \n",
       "min      0.980000    0.340000        0.130000    0.410000      1.280000   \n",
       "25%      1.742500    1.205000        0.270000    1.250000      3.220000   \n",
       "50%      2.355000    2.135000        0.340000    1.555000      4.690000   \n",
       "75%      2.800000    2.875000        0.437500    1.950000      6.200000   \n",
       "max      3.880000    5.080000        0.660000    3.580000     13.000000   \n",
       "\n",
       "              Hue          OD      Proline  \n",
       "count  178.000000  178.000000   178.000000  \n",
       "mean     0.957449    2.611685   746.893258  \n",
       "std      0.228572    0.709990   314.907474  \n",
       "min      0.480000    1.270000   278.000000  \n",
       "25%      0.782500    1.937500   500.500000  \n",
       "50%      0.965000    2.780000   673.500000  \n",
       "75%      1.120000    3.170000   985.000000  \n",
       "max      1.710000    4.000000  1680.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data has several features with min or max values\n",
    "# beyond three standard deviations from the mean - usually\n",
    "# a good indicator of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 int-type columns with NaN values.\n",
      "\n",
      "0 float-type columns with NaN values.\n",
      "\n",
      "0 string-type columns with NaN values.\n"
     ]
    }
   ],
   "source": [
    "# Partition data into features (X-data) and\n",
    "# targets (Y-data).\n",
    "x_data = df.iloc[:,1:]\n",
    "y_data = df.iloc[:,0]\n",
    "\n",
    "# Determine which features have missing values.\n",
    "nan_int_cols = gather_nan(x_data, \"int\")\n",
    "nan_float_cols = gather_nan(x_data, \"float\")\n",
    "nan_string_cols = gather_nan(x_data, \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data is already tidy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and compare various models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #1: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform hyperparameter tuning for a k-Nearest Neighbors (kNN) model using a grid search w/ cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using 50.0% of the data to perform the fit and 50.0% of the data to make the prediction, the model performed according to the following:\n",
      "[0.78947368 0.68421053 0.5        0.76470588 0.5625    ]\n",
      "RMS Error: 0.618\n",
      "Variance score: 0.321\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff476d1f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:\n",
      "{'n_neighbors': 1}\n",
      "with a score of 0.725\n"
     ]
    }
   ],
   "source": [
    "k_values = list(range(1,25))\n",
    "parameter_grid = {\n",
    "    \"n_neighbors\": k_values\n",
    "}\n",
    "model_knn = GridSearchCV(estimator, cv=5, return_train_score=True,\n",
    "    param_grid=parameter_grid)\n",
    "evaluate_model(x_data, y_data, model_knn, False)\n",
    "print(\"The best parameters are:\")\n",
    "print(model_knn.best_params_)\n",
    "print(\"with a score of %.3f\" % model_knn.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #2: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = svm.SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform hyperparameter tuning for a SVM model using a grid search w/ cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using 50.0% of the data to perform the fit and 50.0% of the data to make the prediction, the model performed according to the following:\n",
      "[0.94736842 1.         0.88888889 0.82352941 0.9375    ]\n",
      "RMS Error: 0.212\n",
      "Variance score: 0.920\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff46f691c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:\n",
      "{'coef0': 0.0, 'kernel': 'linear', 'tol': 0.001}\n",
      "with a score of 0.961\n"
     ]
    }
   ],
   "source": [
    "parameter_grid = {\n",
    "    \"kernel\": ['linear', 'rbf'],\n",
    "    \"coef0\": [0.0, 1.0],\n",
    "    \"tol\": [1e-3, 1e-4]\n",
    "}\n",
    "model_svm = GridSearchCV(estimator, cv=5, return_train_score=True,\n",
    "    param_grid=parameter_grid)\n",
    "evaluate_model(x_data, y_data, model_svm, False)\n",
    "print(\"The best parameters are:\")\n",
    "print(model_svm.best_params_)\n",
    "print(\"with a score of %.3f\" % model_svm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both models perform reasonably well when trained on half the data, but the SVM model scored significantly better than the kNN model (0.961 and 0.725, respectively). The SVM is therefore probably the preferred classifier for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How well does the kNN model perform\n",
    "# on the full data set?\n",
    "y_pred = model_knn.predict(x_data)\n",
    "metrics.accuracy_score(y_data, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943820224719101"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How well does the SVM model perform on the full\n",
    "# data set?\n",
    "y_pred = model_svm.predict(x_data)\n",
    "metrics.accuracy_score(y_data, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After being trained on the full data set, both the kNN and the SVM models perfectly (or nearly perfectly) predict all classifications. This is likely a  symptom of overfitting, and both models would need to be cross-validated using additional data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
